{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"-\"\n"
     ]
    }
   ],
   "source": [
    "!pip - q install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharlesattend_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "from tqdm.autonotebook import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Type\n",
    "\n",
    "import torch\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CustomCrossEncoder(CrossEncoder):\n",
    "    def __init__(self, model_name: str, num_labels: int = None, max_length: int = None, device: str = None, tokenizer_args: Dict = {},\n",
    "                 automodel_args: Dict = {}, default_activation_function=None):\n",
    "        super().__init__(model_name, num_labels, max_length, device,\n",
    "                         tokenizer_args, automodel_args, default_activation_function)\n",
    "\n",
    "    def fit(self,\n",
    "            train_dataloader: DataLoader,\n",
    "            evaluator: SentenceEvaluator = None,\n",
    "            epochs: int = 1,\n",
    "            loss_fct=None,\n",
    "            activation_fct=nn.Identity(),\n",
    "            scheduler: str = 'WarmupLinear',\n",
    "            warmup_steps: int = 10000,\n",
    "            optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n",
    "            optimizer_params: Dict[str, object] = {'lr': 2e-5},\n",
    "            weight_decay: float = 0.01,\n",
    "            evaluation_steps: int = 0,\n",
    "            output_path: str = None,\n",
    "            save_best_model: bool = True,\n",
    "            max_grad_norm: float = 1,\n",
    "            use_amp: bool = False,\n",
    "            callback: Callable[[float, int, int], None] = None,\n",
    "            show_progress_bar: bool = True\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Train the model with the given training objective\n",
    "        Each training objective is sampled in turn for one batch.\n",
    "        We sample only as many batches from each objective as there are in the smallest one\n",
    "        to make sure of equal training with each dataset.\n",
    "\n",
    "        :param train_dataloader: DataLoader with training InputExamples\n",
    "        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n",
    "        :param epochs: Number of epochs for training\n",
    "        :param loss_fct: Which loss function to use for training. If None, will use nn.BCEWithLogitsLoss() if self.config.num_labels == 1 else nn.CrossEntropyLoss()\n",
    "        :param activation_fct: Activation function applied on top of logits output of model.\n",
    "        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
    "        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n",
    "        :param optimizer_class: Optimizer\n",
    "        :param optimizer_params: Optimizer parameters\n",
    "        :param weight_decay: Weight decay for model parameters\n",
    "        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n",
    "        :param output_path: Storage path for the model and evaluation files\n",
    "        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n",
    "        :param max_grad_norm: Used for gradient normalization.\n",
    "        :param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n",
    "        :param callback: Callback function that is invoked after each evaluation.\n",
    "                It must accept the following three parameters in this order:\n",
    "                `score`, `epoch`, `steps`\n",
    "        :param show_progress_bar: If True, output a tqdm progress bar\n",
    "        \"\"\"\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"Information Retrieval Project\",\n",
    "            config={\n",
    "                \"epochs\": epochs,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"evaluation_steps\": evaluation_steps,\n",
    "                \"optimizer_params\": optimizer_params,\n",
    "                \"optimizer_class\": optimizer_class.__name__,\n",
    "            }\n",
    "        )\n",
    "        train_dataloader.collate_fn = self.smart_batching_collate\n",
    "\n",
    "        if use_amp:\n",
    "            from torch.cuda.amp import autocast\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        self.model.to(self._target_device)\n",
    "\n",
    "        if output_path is not None:\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        self.best_score = -9999999\n",
    "        num_train_steps = int(len(train_dataloader) * epochs)\n",
    "\n",
    "        # Prepare optimizers\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = optimizer_class(\n",
    "            optimizer_grouped_parameters, **optimizer_params)\n",
    "\n",
    "        if isinstance(scheduler, str):\n",
    "            scheduler = SentenceTransformer._get_scheduler(\n",
    "                optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n",
    "\n",
    "        if loss_fct is None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(\n",
    "            ) if self.config.num_labels == 1 else nn.CrossEntropyLoss()\n",
    "\n",
    "        skip_scheduler = False\n",
    "        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n",
    "            training_steps = 0\n",
    "            self.model.zero_grad()\n",
    "            self.model.train()\n",
    "\n",
    "            for features, labels in tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        model_predictions = self.model(\n",
    "                            **features, return_dict=True)\n",
    "                        logits = activation_fct(model_predictions.logits)\n",
    "                        if self.config.num_labels == 1:\n",
    "                            logits = logits.view(-1)\n",
    "                        loss_value = loss_fct(logits, labels)\n",
    "                    wandb.log({\"loss\": loss_value})\n",
    "                    scale_before_step = scaler.get_scale()\n",
    "                    scaler.scale(loss_value).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    skip_scheduler = scaler.get_scale() != scale_before_step\n",
    "                else:\n",
    "                    model_predictions = self.model(\n",
    "                        **features, return_dict=True)\n",
    "                    logits = activation_fct(model_predictions.logits)\n",
    "                    if self.config.num_labels == 1:\n",
    "                        logits = logits.view(-1)\n",
    "                    loss_value = loss_fct(logits, labels)\n",
    "                    wandb.log({\"loss\": loss_value})\n",
    "                    loss_value.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), max_grad_norm)\n",
    "                    optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if not skip_scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "                training_steps += 1\n",
    "\n",
    "                if evaluator is not None and evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n",
    "                    self._eval_during_training(\n",
    "                        evaluator, output_path, save_best_model, epoch, training_steps, callback)\n",
    "\n",
    "                    self.model.zero_grad()\n",
    "                    self.model.train()\n",
    "\n",
    "            if evaluator is not None:\n",
    "                self._eval_during_training(\n",
    "                    evaluator, output_path, save_best_model, epoch, -1, callback)\n",
    "\n",
    "\n",
    "def eval_callback(score, epoch, step):\n",
    "    wandb.log({'MRR': score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:58:25 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "# /print debug information to stdout\n",
    "\n",
    "\n",
    "# First, we define the transformer model we want to fine-tune\n",
    "model_name = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "train_batch_size = 32\n",
    "num_epochs = 1\n",
    "model_save_path = 'output/training_ms-marco_cross-encoder-' + \\\n",
    "    model_name.replace(\"/\", \"-\")+'-' + \\\n",
    "    datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# We train the network with as a binary label task\n",
    "# Given [query, passage] is the label 0 = irrelevant or 1 = relevant?\n",
    "# We use a positive-to-negative ratio: For 1 positive sample (label 1) we include 4 negative samples (label 0)\n",
    "# in our training setup. For the negative samples, we use the triplets provided by MS Marco that\n",
    "# specify (query, positive sample, negative sample).\n",
    "pos_neg_ration = 4\n",
    "\n",
    "# Maximal number of training samples we want to use\n",
    "max_train_samples = 2e7\n",
    "# max_train_samples = 8_800_000\n",
    "\n",
    "\n",
    "# We set num_labels=1, which predicts a continous score between 0 and 1\n",
    "model = CustomCrossEncoder(model_name, num_labels=1, max_length=512)\n",
    "\n",
    "\n",
    "# Now we read the MS Marco dataset\n",
    "data_folder = './datasets/collectionandqueries'\n",
    "os.makedirs(data_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20.0Mit [01:14, 269kit/s] \n"
     ]
    }
   ],
   "source": [
    "# Read the corpus files, that contain all the passages. Store them in the corpus dict\n",
    "corpus = {}\n",
    "collection_filepath = os.path.join(data_folder, 'collection.tsv')\n",
    "if not os.path.exists(collection_filepath):\n",
    "    tar_filepath = os.path.join(data_folder, 'collection.tar.gz')\n",
    "    if not os.path.exists(tar_filepath):\n",
    "        logging.info(\"Download collection.tar.gz\")\n",
    "        util.http_get(\n",
    "            'https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz', tar_filepath)\n",
    "\n",
    "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_folder)\n",
    "\n",
    "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        pid, passage = line.strip().split(\"\\t\")\n",
    "        corpus[pid] = passage\n",
    "\n",
    "\n",
    "# Read the train queries, store in queries dict\n",
    "queries = {}\n",
    "queries_filepath = os.path.join(data_folder, 'queries.train.tsv')\n",
    "if not os.path.exists(queries_filepath):\n",
    "    tar_filepath = os.path.join(data_folder, 'queries.tar.gz')\n",
    "    if not os.path.exists(tar_filepath):\n",
    "        logging.info(\"Download queries.tar.gz\")\n",
    "        util.http_get(\n",
    "            'https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz', tar_filepath)\n",
    "\n",
    "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_folder)\n",
    "\n",
    "\n",
    "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        qid, query = line.strip().split(\"\\t\")\n",
    "        queries[qid] = query\n",
    "\n",
    "\n",
    "# Now we create our training & dev data\n",
    "train_samples = []\n",
    "dev_samples = {}\n",
    "\n",
    "# We use 200 random queries from the train set for evaluation during training\n",
    "# Each query has at least one relevant and up to 200 irrelevant (negative) passages\n",
    "num_dev_queries = 200\n",
    "num_max_dev_negatives = 200\n",
    "\n",
    "# msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz and msmarco-qidpidtriples.rnd-shuf.train.tsv.gz is a randomly\n",
    "# shuffled version of qidpidtriples.train.full.2.tsv.gz from the MS Marco website\n",
    "# We extracted in the train-eval split 500 random queries that can be used for evaluation during training\n",
    "train_eval_filepath = os.path.join(\n",
    "    data_folder, 'msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz')\n",
    "if not os.path.exists(train_eval_filepath):\n",
    "    logging.info(\"Download \"+os.path.basename(train_eval_filepath))\n",
    "    util.http_get(\n",
    "        'https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz', train_eval_filepath)\n",
    "\n",
    "with gzip.open(train_eval_filepath, 'rt') as fIn:\n",
    "    for line in fIn:\n",
    "        qid, pos_id, neg_id = line.strip().split()\n",
    "\n",
    "        if qid not in dev_samples and len(dev_samples) < num_dev_queries:\n",
    "            dev_samples[qid] = {'query': queries[qid],\n",
    "                                'positive': set(), 'negative': set()}\n",
    "\n",
    "        if qid in dev_samples:\n",
    "            dev_samples[qid]['positive'].add(corpus[pos_id])\n",
    "\n",
    "            if len(dev_samples[qid]['negative']) < num_max_dev_negatives:\n",
    "                dev_samples[qid]['negative'].add(corpus[neg_id])\n",
    "\n",
    "\n",
    "# Read our training file\n",
    "train_filepath = os.path.join(\n",
    "    data_folder, 'msmarco-qidpidtriples.rnd-shuf.train.tsv.gz')\n",
    "if not os.path.exists(train_filepath):\n",
    "    logging.info(\"Download \"+os.path.basename(train_filepath))\n",
    "    util.http_get(\n",
    "        'https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train.tsv.gz', train_filepath)\n",
    "\n",
    "cnt = 0\n",
    "with gzip.open(train_filepath, 'rt') as fIn:\n",
    "    for line in tqdm(fIn, unit_scale=True):\n",
    "        qid, pos_id, neg_id = line.strip().split()\n",
    "\n",
    "        if qid in dev_samples:\n",
    "            continue\n",
    "\n",
    "        query = queries[qid]\n",
    "        if (cnt % (pos_neg_ration+1)) == 0:\n",
    "            passage = corpus[pos_id]\n",
    "            label = 1\n",
    "        else:\n",
    "            passage = corpus[neg_id]\n",
    "            label = 0\n",
    "\n",
    "        train_samples.append(InputExample(texts=[query, passage], label=label))\n",
    "        cnt += 1\n",
    "\n",
    "        if cnt >= max_train_samples:\n",
    "            break\n",
    "\n",
    "# We create a DataLoader to load our train samples\n",
    "train_dataloader = DataLoader(\n",
    "    train_samples, shuffle=True, batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:58:28 - Warmup-steps: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jn4tpn2u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-plasma-5</strong> at: <a href='https://wandb.ai/charlesattend_/Information%20Retrieval%20Project/runs/jn4tpn2u' target=\"_blank\">https://wandb.ai/charlesattend_/Information%20Retrieval%20Project/runs/jn4tpn2u</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230509_145738-jn4tpn2u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jn4tpn2u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tempory/Information-Retrieval-Project/wandb/run-20230509_145828-nxhquq3p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/charlesattend_/Information%20Retrieval%20Project/runs/nxhquq3p' target=\"_blank\">comfy-water-6</a></strong> to <a href='https://wandb.ai/charlesattend_/Information%20Retrieval%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/charlesattend_/Information%20Retrieval%20Project' target=\"_blank\">https://wandb.ai/charlesattend_/Information%20Retrieval%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/charlesattend_/Information%20Retrieval%20Project/runs/nxhquq3p' target=\"_blank\">https://wandb.ai/charlesattend_/Information%20Retrieval%20Project/runs/nxhquq3p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \n",
      "Epoch:   0%|          | 0/1 [00:12<?, ?it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:58:48 - CERerankingEvaluator: Evaluating the model on train-eval dataset in epoch 0 after 100 steps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \n",
      "                                                                 \n",
      "                                                                 \n",
      "Epoch:   0%|          | 0/1 [01:17<?, ?it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:59:53 - Queries: 200 \t Positives: Min 1.0, Mean 1.1, Max 3.0 \t Negatives: Min 100.0, Mean 199.1, Max 200.0\n",
      "2023-05-09 14:59:53 - MRR@10: 1.58\n",
      "2023-05-09 14:59:53 - Save model to output/training_ms-marco_cross-encoder-microsoft-MiniLM-L12-H384-uncased-2023-05-09_14-58-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                            \n",
      "Epoch:   0%|          | 0/1 [01:29<?, ?it/s]                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 15:00:05 - CERerankingEvaluator: Evaluating the model on train-eval dataset in epoch 0 after 200 steps:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 199/625000 [01:41<88:37:06,  1.96it/s]\n",
      "Epoch:   0%|          | 0/1 [01:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mWarmup-steps: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(warmup_steps))\n\u001b[1;32m     10\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     12\u001b[0m           evaluator\u001b[39m=\u001b[39;49mevaluator,\n\u001b[1;32m     13\u001b[0m           epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m     14\u001b[0m           loss_fct\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m     15\u001b[0m           evaluation_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m           warmup_steps\u001b[39m=\u001b[39;49mwarmup_steps,\n\u001b[1;32m     17\u001b[0m           output_path\u001b[39m=\u001b[39;49mmodel_save_path,\n\u001b[1;32m     18\u001b[0m           use_amp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     19\u001b[0m           optimizer_class\u001b[39m=\u001b[39;49mAdam,\n\u001b[1;32m     20\u001b[0m           optimizer_params\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m7e-6\u001b[39;49m},\n\u001b[1;32m     21\u001b[0m           callback\u001b[39m=\u001b[39;49meval_callback)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Save latest model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[39m.\u001b[39msave(model_save_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-latest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 153\u001b[0m, in \u001b[0;36mCustomCrossEncoder.fit\u001b[0;34m(self, train_dataloader, evaluator, epochs, loss_fct, activation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar)\u001b[0m\n\u001b[1;32m    150\u001b[0m training_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m evaluator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m evaluation_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m training_steps \u001b[39m%\u001b[39m evaluation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_eval_during_training(\n\u001b[1;32m    154\u001b[0m         evaluator, output_path, save_best_model, epoch, training_steps, callback)\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    157\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:299\u001b[0m, in \u001b[0;36mCrossEncoder._eval_during_training\u001b[0;34m(self, evaluator, output_path, save_best_model, epoch, steps, callback)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs evaluation during the training\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m evaluator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     score \u001b[39m=\u001b[39m evaluator(\u001b[39mself\u001b[39;49m, output_path\u001b[39m=\u001b[39;49moutput_path, epoch\u001b[39m=\u001b[39;49mepoch, steps\u001b[39m=\u001b[39;49msteps)\n\u001b[1;32m    300\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m         callback(score, epoch, steps)\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/evaluation/CERerankingEvaluator.py:60\u001b[0m, in \u001b[0;36mCERerankingEvaluator.__call__\u001b[0;34m(self, model, output_path, epoch, steps)\u001b[0m\n\u001b[1;32m     57\u001b[0m num_negatives\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(negative))\n\u001b[1;32m     59\u001b[0m model_input \u001b[39m=\u001b[39m [[query, doc] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m---> 60\u001b[0m pred_scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(model_input, convert_to_numpy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     61\u001b[0m pred_scores_argsort \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(\u001b[39m-\u001b[39mpred_scores)  \u001b[39m#Sort in decreasing order\u001b[39;00m\n\u001b[1;32m     63\u001b[0m mrr_score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:274\u001b[0m, in \u001b[0;36mCrossEncoder.predict\u001b[0;34m(self, sentences, batch_size, show_progress_bar, num_workers, activation_fct, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_target_device)\n\u001b[1;32m    273\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mfor\u001b[39;00m features \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m    275\u001b[0m         model_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfeatures, return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m         logits \u001b[39m=\u001b[39m activation_fct(model_predictions\u001b[39m.\u001b[39mlogits)\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/tempory/Information-Retrieval-Project/.venv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:99\u001b[0m, in \u001b[0;36mCrossEncoder.smart_batching_collate_text_only\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     96\u001b[0m tokenized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\u001b[39m*\u001b[39mtexts, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest_first\u001b[39m\u001b[39m'\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m tokenized:\n\u001b[0;32m---> 99\u001b[0m     tokenized[name] \u001b[39m=\u001b[39m tokenized[name]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_device)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We add an evaluator, which evaluates the performance during training\n",
    "# It performs a classification task and measures scores like F1 (finding relevant passages) and Average Precision\n",
    "evaluator = CERerankingEvaluator(dev_samples, name='train-eval')\n",
    "\n",
    "# Configure the training\n",
    "warmup_steps = 5000\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          loss_fct=nn.CrossEntropyLoss(),\n",
    "          evaluation_steps=100,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path,\n",
    "          use_amp=True,\n",
    "          optimizer_class=Adam,\n",
    "          optimizer_params={'lr': 7e-6},\n",
    "          callback=eval_callback)\n",
    "\n",
    "# Save latest model\n",
    "model.save(model_save_path+'-latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
